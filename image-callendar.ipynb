{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":9638229,"datasetId":5884892,"databundleVersionId":9864016}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#pip install paddlepaddle\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T07:42:37.394749Z","iopub.execute_input":"2024-10-16T07:42:37.395096Z","iopub.status.idle":"2024-10-16T07:42:37.580344Z","shell.execute_reply.started":"2024-10-16T07:42:37.395047Z","shell.execute_reply":"2024-10-16T07:42:37.579328Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# pip install paddleocr","metadata":{"execution":{"iopub.status.busy":"2024-10-16T07:42:37.581893Z","iopub.execute_input":"2024-10-16T07:42:37.582194Z","iopub.status.idle":"2024-10-16T07:42:37.658848Z","shell.execute_reply.started":"2024-10-16T07:42:37.582156Z","shell.execute_reply":"2024-10-16T07:42:37.657926Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# pip install transformers torch numpy","metadata":{"execution":{"iopub.status.busy":"2024-10-16T09:07:04.729686Z","iopub.execute_input":"2024-10-16T09:07:04.730751Z","iopub.status.idle":"2024-10-16T09:07:04.735324Z","shell.execute_reply.started":"2024-10-16T09:07:04.730696Z","shell.execute_reply":"2024-10-16T09:07:04.734367Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"import pytesseract\nfrom PIL import Image\nimport easyocr\n# with torch.amp.autocast('cuda'):\nreader = easyocr.Reader(['ko', 'en'])  # 한글과 영어 설정\nresult = reader.readtext('/kaggle/input/posterimage/.png')\nfor r in result:\n    print(r[1])  # 추출된 텍스트 출력\n\n    #빠르지만 정확도가 다소 떨어짐","metadata":{"execution":{"iopub.status.busy":"2024-10-16T07:42:49.243111Z","iopub.execute_input":"2024-10-16T07:42:49.243455Z","iopub.status.idle":"2024-10-16T07:42:59.734574Z","shell.execute_reply.started":"2024-10-16T07:42:49.243419Z","shell.execute_reply":"2024-10-16T07:42:59.733480Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"},{"name":"stdout","text":"00:10\nLTE\n83\n[서울]소비자가바꾸스세상(0713-001)\n과목공지 (조회)\n제목: 여러분 공모전 안내입니다\n계시일: 어제 오후 213\n여러분! 안녕하세요?\n유난히 무더워던 여름도 지나가고 선선한 가을을 기다리면서 .\n공모전올 안내해 드립니다\n도전 바라면서 .. 철부파일도 올리니 참고해 주세요\n혹여 도움이 필요하면 연락주세요.\n김시월 드림\n2024 시니어 소비자교육 존렌츠 공모전-\n주제: 디지털 소비환경에서의 시니어 소비자교육 소렌츠\n예시: 인터넷 쇼풍, 홍소굉 이용 방법\n인터넷 금움거래 방법\n모바일 입 이용하여 거래하는 방법 등\n형식: 동영상(2-3분) , 카드뉴스(8-10장)\n대상: 대학(원)생(휴학생 포함) (개인 또는 4명 이내 팀 가능)\n제출 마감: 2024년 11월 29일, 온라인으로 파일제출\n상금: 총 상금 500만원\n0\n2024 소비자교육 밖렌l공모전 포스터 최종pdf (1.17 MB)\n대글 0\n모로\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# 정확도는 더 높으나 속도가 조금 느리다.\n# 필요한 라이브러리 임포트\nimport paddleocr\nimport logging\n\n# 로그 레벨 설정\nlogging.getLogger(\"ppocr\").setLevel(logging.WARNING)\n\n# 한글과 영어 추출을 위한 OCR 객체 생성\nocr = paddleocr.PaddleOCR(lang='korean')\n\n# 이미지에서 텍스트 추출\nresult = ocr.ocr('/kaggle/input/posterimage/.png', cls=True)\n\n# 텍스트를 줄별로 저장할 리스트\nextracted_lines = []\n\n# y좌표의 범위 설정 (오차)\ny_threshold = 30  # 예: 10픽셀 이내의 차이\n\n# 각 줄의 텍스트를 저장할 변수\nline_text = \"\"\n\n# 이전 y좌표를 저장하기 위한 변수\nprevious_y = None\n\n# 결과를 순회하며 줄 텍스트를 구성\nfor line in result:\n    for word in line:\n        word_text = word[1][0]  # 단어 텍스트\n        y_coordinate = (word[0][0][1] + word[0][2][1]) / 2  # 단어의 y좌표 (중간값)\n\n        # 첫 단어일 경우 또는 이전 y좌표가 None인 경우\n        if previous_y is None:\n            line_text += word_text + \" \"\n            previous_y = y_coordinate\n        else:\n            # 이전 y좌표와 현재 y좌표의 차이 계산\n            if abs(y_coordinate - previous_y) <= y_threshold:\n                line_text += word_text + \" \"\n            else:\n                # y좌표 차이가 threshold를 넘으면 현재 줄을 리스트에 추가하고 line_text 초기화\n                extracted_lines.append(line_text.strip())  # 앞뒤 공백 제거하여 리스트에 추가\n                line_text = word_text + \" \"  # 새로운 줄 시작\n                previous_y = y_coordinate  # 이전 y좌표 업데이트\n\n# 마지막으로 남아있는 줄 텍스트 추가\nif line_text:\n    extracted_lines.append(line_text.strip())\n\n# 최종적으로 추출된 텍스트 리스트 출력\nprint(extracted_lines)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T09:10:38.137996Z","iopub.execute_input":"2024-10-16T09:10:38.138894Z","iopub.status.idle":"2024-10-16T09:10:48.999655Z","shell.execute_reply.started":"2024-10-16T09:10:38.138849Z","shell.execute_reply":"2024-10-16T09:10:48.998581Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"[2024/10/16 09:10:41] ppocr WARNING: Since the angle classifier is not initialized, it will not be used during the forward process\n['00:10 11 LTE 83', '[서울]소비자가바꾸는세상0713-001', '과목공지 조회', '제목 :여러분! 공모전 안내입니다', '게시일 :어제오후2:13', '여러분! 안녕하세요?', '유난히 무더웠던 여름도 지나가고 선선한 가을을 기다리면서', '공모전을 안내해 드립니다', '바라면서첨부파일도 도전 올리니 참고해 주세요', '혹여 도움이 필요하면 연락주세요', '김시월 드림', '2024 시니어 소비자교육 콘텐츠 공모전-', '주제: 디지털 소비환경에서의 시니어 소비자교육 콘텐츠', '예시: 인터넷 쇼핑 홈쇼핑 이용 방법', '인터넷 금융거래 방법', '모바일 앱 이용하여 거래하는 방법동 등', '형식: 동영상2-3분 카드뉴스8-10장', '대상: 대학원생휴학생 포함개인 또는 4명 이내 팀 가능', '제출 마감: 2024년 11월 29일 온라인으로파일제출', '상금: 총상금 500만원', '1 2024 소비자교육 콘텐츠공모전 포스터 최종pdf 117 MB', '덧글 0', '모로']\n","output_type":"stream"}]},{"cell_type":"code","source":"#정규 표현식으로 찾기\nimport re\nall_dates = []\ndate_pattern = r\"\\d{4}년 \\d{1,2}월 \\d{1,2}일\"\nfor line in extracted_lines:\n    dates = re.findall(date_pattern, line)\n    all_dates.extend(dates)\n\n# 찾은 날짜 출력\nprint(all_dates)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T09:10:16.629432Z","iopub.execute_input":"2024-10-16T09:10:16.630238Z","iopub.status.idle":"2024-10-16T09:10:16.635752Z","shell.execute_reply.started":"2024-10-16T09:10:16.630194Z","shell.execute_reply":"2024-10-16T09:10:16.634807Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"['2024년 11월 29일']\n","output_type":"stream"}]},{"cell_type":"code","source":"# from transformers import AutoModelForCausalLM, AutoTokenizer\n# import torch\n\n# # 모델과 토크나이저 로드\n# model_name = \"yanolja/EEVE-Korean-10.8B-v1.0\"\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n# model = AutoModelForCausalLM.from_pretrained(model_name)\n\n# # GPU 사용 여부에 따른 설정\n# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# model.to(device)\n\n# # 입력 텍스트\n# input_text = \"안녕하세요, 오늘 날씨는 어떤가요?\"\n# input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\n# # 텍스트 생성\n# with torch.no_grad():\n#     output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n\n# # 생성된 텍스트 출력\n# generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n# print(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T07:49:27.444682Z","iopub.execute_input":"2024-10-16T07:49:27.445099Z","iopub.status.idle":"2024-10-16T07:49:27.450050Z","shell.execute_reply.started":"2024-10-16T07:49:27.445050Z","shell.execute_reply":"2024-10-16T07:49:27.449082Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# from transformers import pipeline\n# #NER모델로 뽑아보기 안된다..\n# # NER 모델 파이프라인 생성\n# # GPU 사용 설정\n# nlp = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", device=0)\n\n# # 텍스트에 대해 네임드 엔티티 인식(NER) 수행\n# results = nlp(extracted_text)\n\n# # 일정과 관련된 엔티티(날짜, 제목) 필터링\n# events = [(entity['word'], entity['entity']) for entity in results if entity['entity'] in ['DATE', 'EVENT']]\n\n# # 결과 출력\n# print(\"추출된 이벤트 및 날짜:\")\n# for event in events:\n#     print(event)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T07:51:40.749646Z","iopub.execute_input":"2024-10-16T07:51:40.750611Z","iopub.status.idle":"2024-10-16T07:51:41.822251Z","shell.execute_reply.started":"2024-10-16T07:51:40.750570Z","shell.execute_reply":"2024-10-16T07:51:41.821182Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"추출된 이벤트 및 날짜:\n","output_type":"stream"}]},{"cell_type":"code","source":"#kor electra model 로드 \nfrom transformers import ElectraTokenizer, ElectraForTokenClassification\nimport torch\n\n# ELECTRA 모델과 토크나이저 로드\nmodel_name = \"kykim/electra-kor-base\"  # ELECTRA 기반 한국어 모델 이름\ntokenizer = ElectraTokenizer.from_pretrained(model_name)\nmodel = ElectraForTokenClassification.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T08:12:15.629717Z","iopub.execute_input":"2024-10-16T08:12:15.630140Z","iopub.status.idle":"2024-10-16T08:12:26.250650Z","shell.execute_reply.started":"2024-10-16T08:12:15.630103Z","shell.execute_reply":"2024-10-16T08:12:26.249873Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/80.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a09f5244df4e4f95819cbeddabed7356"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/344k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da9940922fa145ce81e3ede742f597e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/620 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb43d9f9f4904048a39aec7e21e7c350"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/473M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce98cff06cf04b2d811571cd3ab61f55"}},"metadata":{}},{"name":"stderr","text":"Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at kykim/electra-kor-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# 텍스트를 토큰화하고 텐서로 변환\ninputs = tokenizer(extracted_lines, return_tensors=\"pt\")\n\n# 모델 예측\nwith torch.no_grad():\n    outputs = model(**inputs)\n    logits = outputs.logits\n\n# 예측된 클래스 (엔티티 태그) 결정\npredicted_classes = torch.argmax(logits, dim=2)\n\n# 결과 출력\nfor token, pred in zip(inputs['input_ids'][0], predicted_classes[0]):\n    print(tokenizer.decode(token), pred.item())\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T08:27:41.749093Z","iopub.execute_input":"2024-10-16T08:27:41.749997Z","iopub.status.idle":"2024-10-16T08:27:42.266838Z","shell.execute_reply.started":"2024-10-16T08:27:41.749955Z","shell.execute_reply":"2024-10-16T08:27:42.265392Z"},"trusted":true},"execution_count":33,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:775\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 775\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:737\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 737\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mValueError\u001b[0m: expected sequence of length 8 at dim 1 (got 16)","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 텍스트를 토큰화하고 텐서로 변환\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextracted_lines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 모델 예측\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3024\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   3023\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 3024\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3112\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3107\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3108\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3109\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3110\u001b[0m         )\n\u001b[1;32m   3111\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 3112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3114\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3130\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3132\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   3135\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   3136\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3154\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3155\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3314\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3304\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3305\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3306\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3307\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3311\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3312\u001b[0m )\n\u001b[0;32m-> 3314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3316\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3332\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3333\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3334\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils.py:889\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    886\u001b[0m     second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     input_ids\u001b[38;5;241m.\u001b[39mappend((first_ids, second_ids))\n\u001b[0;32m--> 889\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_prepare_for_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchEncoding(batch_outputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils.py:976\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_prepare_for_model\u001b[0;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    965\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[1;32m    967\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    968\u001b[0m     batch_outputs,\n\u001b[1;32m    969\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding_strategy\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    973\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m    974\u001b[0m )\n\u001b[0;32m--> 976\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:240\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    236\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:791\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    788\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    789\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    790\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    792\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    793\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    794\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n","\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."],"ename":"ValueError","evalue":"Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).","output_type":"error"}]},{"cell_type":"code","source":"# 예시 엔티티 필터링 (사용할 엔티티에 맞게 수정)\nentities = [(tokenizer.decode(token), label.item()) for token, label in zip(inputs['input_ids'][0], predicted_classes[0]) if label in [0, 1]]  # 0과 1은 예시입니다.\n\n# 결과 출력\nfor entity in entities:\n    print(entity)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T08:14:26.570604Z","iopub.execute_input":"2024-10-16T08:14:26.571014Z","iopub.status.idle":"2024-10-16T08:14:26.590445Z","shell.execute_reply.started":"2024-10-16T08:14:26.570976Z","shell.execute_reply":"2024-10-16T08:14:26.589539Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"('[CLS]', 1)\n('00', 0)\n(':', 0)\n('10', 0)\n('11', 0)\n('lte', 0)\n('83', 0)\n('[', 0)\n('서울', 0)\n(']', 0)\n('소비자가', 0)\n('##바', 0)\n('##꾸', 0)\n('##는', 0)\n('##세상', 0)\n('##0', 0)\n('##71', 0)\n('##3', 0)\n('-', 0)\n('00', 0)\n('##1', 0)\n('과목', 0)\n('##공', 0)\n('##지', 0)\n('조회', 0)\n('제목', 0)\n(':', 0)\n('여러분', 0)\n('!', 0)\n('공모', 0)\n('##전', 0)\n('안내', 0)\n('##입니다', 0)\n('게시', 0)\n('##일', 0)\n(':', 0)\n('어제', 0)\n('##오', 0)\n('##후', 0)\n('##2', 0)\n(':', 0)\n('13', 0)\n('여러분', 0)\n('!', 0)\n('안녕하세요', 0)\n('?', 0)\n('유난히', 0)\n('무더', 0)\n('##웠던', 0)\n('여름', 0)\n('##도', 0)\n('지나가고', 0)\n('선선', 0)\n('##한', 0)\n('가을', 0)\n('##을', 0)\n('기다리', 0)\n('##면서', 0)\n('공모', 0)\n('##전을', 0)\n('안내', 0)\n('##해', 0)\n('드립니다', 0)\n('바라', 0)\n('##면서', 0)\n('##첨', 0)\n('##부', 0)\n('##파일', 0)\n('##도', 0)\n('도전', 0)\n('올리', 0)\n('##니', 0)\n('참고해', 0)\n('주세요', 0)\n('혹여', 0)\n('도움이', 0)\n('필요', 0)\n('##하면', 0)\n('연락주세요', 0)\n('김', 0)\n('##시', 0)\n('##월', 0)\n('드림', 0)\n('202', 0)\n('##4', 0)\n('시', 0)\n('##니어', 0)\n('소비자', 0)\n('##교육', 0)\n('콘텐츠', 0)\n('공모', 0)\n('##전', 0)\n('-', 0)\n('주제', 0)\n(':', 0)\n('디지털', 0)\n('소비', 0)\n('##환경', 0)\n('##에서의', 0)\n('시', 0)\n('##니어', 0)\n('소비자', 0)\n('##교육', 0)\n('콘텐츠', 0)\n('예시', 0)\n(':', 0)\n('인터넷', 0)\n('쇼핑', 0)\n('홈쇼핑', 0)\n('이용', 0)\n('방법', 0)\n('인터넷', 0)\n('금융', 0)\n('##거래', 0)\n('방법', 0)\n('모바일', 0)\n('앱', 0)\n('이용하여', 0)\n('거래', 0)\n('##하는', 0)\n('방법', 0)\n('##동', 0)\n('등', 0)\n('형식', 0)\n(':', 0)\n('동영상', 0)\n('##2', 0)\n('-', 0)\n('3분', 0)\n('카드', 0)\n('##뉴스', 0)\n('##8', 0)\n('-', 0)\n('10', 0)\n('##장', 0)\n('대상', 0)\n(':', 0)\n('대학원', 0)\n('##생', 0)\n('##휴', 0)\n('##학생', 0)\n('포함', 0)\n('##개인', 0)\n('또는', 0)\n('4명', 0)\n('이내', 0)\n('팀', 0)\n('가능', 0)\n('제출', 0)\n('마감', 0)\n(':', 0)\n('202', 0)\n('##4년', 0)\n('11월', 0)\n('29일', 0)\n('온라인으로', 0)\n('##파일', 0)\n('##제', 0)\n('##출', 0)\n('상', 0)\n('##금', 0)\n(':', 0)\n('총', 0)\n('##상', 0)\n('##금', 0)\n('500만', 0)\n('##원', 0)\n('1', 0)\n('202', 0)\n('##4', 0)\n('소비자', 0)\n('##교육', 0)\n('콘텐츠', 0)\n('##공', 0)\n('##모', 0)\n('##전', 0)\n('포스터', 0)\n('최종', 0)\n('##p', 0)\n('##d', 0)\n('##f', 0)\n('11', 0)\n('##7', 0)\n('mb', 0)\n('덧글', 0)\n('0', 0)\n('모로', 0)\n('[SEP]', 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}